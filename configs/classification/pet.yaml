model:
  task: classification # Task type: classification
  # timm-swin_base_patch4_window7_224.ms_in22k_ft_in1k: # imgsz 224
  # timm-vit_base_patch8_224.dino: # imgsz 224
  # timm-vit_base_patch16_224.augreg2_in21k_ft_in1k # imgsz 224
  # timm-vit_large_patch16_224.mae: # imgsz 224
  # timm-vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k: # imgsz 224
  # timm-swinv2_large_window12to16_192to256.ms_in22k_ft_in1k: # imgsz 256
  # timm-vit_base_patch16_clip_224.laion2b_ft_in1k: # imgsz 224
  # timm-swinv2_cr_small_ns_224.sw_in1k: # imgsz 224
  # timm-vit_so400m_patch14_siglip_224.webli: # imgsz 224
  # timm-wide_resnet101_2.tv2_in1k: # imgsz 224
  # timm-tf_mobilenetv3_large_minimal_100.in1k: # imgsz 224
  # timm-vit_large_patch14_dinov2.lvd142m: # imgsz 518
  load_from: null
  name: timm-swin_base_patch4_window7_224.ms_in22k_ft_in1k # Model used please refer to timm.models
  image_size: &resize_size 224 # Input image size, using anchor for later reference
  kwargs: {} # Additional parameters passed to model initialization
  num_classes: 35 # Number of classification categories
  pretrained: True # Whether to use pre-trained weights
  backbone_freeze: False # Whether to freeze the backbone network
  bn_freeze: False # Whether to freeze Batch Normalization layers
  bn_freeze_affine: False # Whether to freeze affine transformation parameters of BN layers
  attention_pool: False # Whether to use attention pooling
data:
  # Choose ONE of the following data sources:

  # 1. HuggingFace Dataset (Single-label)
  root: wuji3/oxford-iiit-pet

  # 2. Local Dataset (Single-label)
  # root: <path/to/dataset>  # Format: path/to/dataset with train/ and val/ subdirs

  # 3. CSV Dataset (Multi-label)
  # root: <Data>.csv  # Format: CSV file with image_path and label columns

  nw: 16 # if not multi-nw, set to 0
  train:
    bs: 80 # Batch size per GPU
    base_aug: null # Base data augmentation
    class_aug: null # Class-specific data augmentation
      # Abyssinian: [1,2,3,4]
      # Birman: [1,2,3,4]
    augment: # Data augmentation strategy, refer to utils/augment.py
       - random_choice:
          transforms: 
            - random_color_jitter:
                brightness: 0.1
                contrast: 0.1
                saturation: 0.1
                hue: 0.1
            - random_cutout:
                n_holes: 3
                length: 12
                prob: 0.1
                color: [0, 255]
            - random_gaussianblur:
                kernel_size: 5
            - random_rotate:
                degrees: 10
            - random_autocontrast:
                p: 0.5
            - random_adjustsharpness:
                p: 0.5
            - random_augmix: 
                severity: 3
       - random_horizonflip: 
           p: 0.5
       - random_choice:
          transforms:
            - resize_and_padding: 
                size: *resize_size
                training: True
            - random_crop_and_resize:
                size: *resize_size
                scale: [0.7, 1]
          p: [0.9, 0.1]
       - to_tensor: no_params
       - normalize: 
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
    aug_epoch: 14 # Number of epochs to apply data augmentation
  val:
    bs: 320
    augment:
        - resize_and_padding:
            size: *resize_size
            training: False
        - to_tensor: no_params
        - normalize: 
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
hyp:
  epochs: 15 # Total number of training epochs
  lr0: 0.006 # Initial learning rate
  lrf_ratio: null # Learning rate decay ratio, null means using default value 0.1
  momentum: 0.937 # Optimizer momentum
  weight_decay: 0.0005 # Weight decay
  warmup_momentum: 0.8 # Momentum during warmup phase
  warm_ep: 1 # Number of warmup epochs
  loss:
    ce: True # Whether to use cross-entropy loss
    bce: # Binary cross-entropy, config: [use or not, weight, multi-label or not]
      - False 
      - [0.5, 0.5, 0.5, 0.5, 0.5] # tag1, tag2, tag3, tag4, tag5
  label_smooth: 0.05 # Label smoothing coefficient
  strategy:
    prog_learn: False # Whether to use progressive learning
    mixup: # Mixup data augmentation configuration
      ratio: 0.0
      duration: 10
    focal: # Only With BCE, Focal Loss, config: [use or not, alpha, gamma]
      - False
      - 0.25
      - 1.5
    ohem: # Only With CE, Online Hard Example Mining, config: [use or not, min_kept, thresh_prob, ignore_index]
      - False
      - 8 
      - 0.7 
      - 255 
  optimizer: 
    - sgd # Optimizer type: SGD, Adam, or SAM
    - False # Whether to set different learning rates for different layers of the model
  scheduler: cosine_with_warm # Learning rate scheduler: cosine annealing with warmup